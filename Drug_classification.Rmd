---
title: "Drug_Classification"
author: "Hanyu Chen"
date: '2023-02-08'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("caret")
library("caret")
```
```{r load the data}
data<- read.csv("drug200.csv")
summary(data)
```
```{r Check if there is any missing data}
which(is.na(data)) # no missing value
```


```{r Check with the Data}
dim(data) # shape of (162,6)
sapply(data,class) # check the data type of each column
head(data)
```
```{r levels}
data$Drug <-factor(data$Drug)
data$BP <- factor (data$BP)
data$Cholesterol <- factor(data$Cholesterol)
data$Sex <- factor(data$Sex)
summary(data)
```


Visulization
```{r Univariate Plots}
# Identify x and y
x <- data[,1:5]
y <- data[,6]
# boxplots to see distribution
par(mfrow = c(1:2))
boxplot(x$Age)
boxplot(x$Na_to_K)
```
```{r}
par(mfrow=c(1,3))
title <- c("Sex","BP","Cholesterol")
  for (i in x){
    if (class(i) =="factor"){
      barplot(table(i))
  }
  }

```


```{r Convert Categorical to dummies}
#install.packages("fastDummies")
library("fastDummies")
data <- dummy_cols(data, select_columns = c("Sex","Cholesterol","BP"))
data <- subset(data,select = -c(Sex, Cholesterol, BP))
```


```{r Create a Validation Dataset}
# Randomly create a list of 80% of the index that used for training
validation_index <- createDataPartition(data$Drug, p = 0.80, list = FALSE)
# 20% of the data used for validation
validation <- data[-validation_index,]
# 80% used to train and test
data<- data[validation_index,]
```


```{r 10-fold cross validation}
control <- trainControl(method = 'cv',
                        summaryFunction = defaultSummary,
                        number = 10,
                        savePredictions = TRUE)
metric <- 'Accuracy'
```

Build Models
```{r KNN}
set.seed(37)
knn_m <- train(Drug~., 
               data = data,
               method = "knn", 
               metric = metric, 
               trControl = control,
               tuneGrid = data.frame(k = seq(10,30,by = 1))) # Cross-Validation)
knn_m

```
```{r Logistic Regression}
set.seed(37)
glm_m <- train(Drug~., 
               data = data,
               method = "glmnet", 
               metric = metric, 
               trControl = control
              )
glm_m
plot(glm_m)
```

```{r Random Forest}
set.seed(37)
rf_m <- train(Drug~., 
               data = data,
               method = "rf", 
               metric = metric, 
               trControl = control,
              tuneLength = 30
              )
# mtry = 3 
print(rf_m)
plot(rf_m)
```
```{r Compare all models}
results <- resamples (list(knn = knn_m, rf = rf_m, glm = glm_m))
print(summary(results))
dotplot(results) # Random Forest is the best
```

```{r Using rf for prediction}
pred <- predict(rf_m, validation)
confusionMatrix(pred,validation$Drug)
```
Random Forest model gets an accuracy of 0.975

